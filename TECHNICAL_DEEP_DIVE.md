# ğŸ”¬ Technical Deep Dive

> Step-by-step execution flow of every component in Vantage

---

## Table of Contents

1. [Application Startup](#application-startup)
2. [Search Execution Flow](#search-execution-flow)
3. [Document Ingestion Flow](#document-ingestion-flow)
4. [Agent Execution Details](#agent-execution-details)
5. [Frontend Component Flow](#frontend-component-flow)
6. [Database Schemas](#database-schemas)
7. [API Request/Response Examples](#api-requestresponse-examples)
8. [Error Handling & Fallbacks](#error-handling--fallbacks)

---

## Application Startup

### Backend Initialization Sequence

```python
# backend/api.py - on startup

1. LOAD CONFIGURATION
   â”œâ”€â”€ Read config.yaml
   â”œâ”€â”€ Parse model settings (ollama, embedding, reranker)
   â””â”€â”€ Parse search settings (hybrid weights, reranker threshold)

2. INITIALIZE OPENSEARCH CLIENT
   â”œâ”€â”€ Connect to OpenSearch (localhost:9200)
   â”œâ”€â”€ Verify cluster health
   â””â”€â”€ Create index if not exists (locallens_docs)

3. INITIALIZE EMBEDDING MODEL
   â”œâ”€â”€ Load SentenceTransformer (nomic-ai/nomic-embed-text-v1.5)
   â”œâ”€â”€ Detect CUDA/CPU device
   â””â”€â”€ Log embedding dimension (768)

4. INITIALIZE RERANKER
   â”œâ”€â”€ Load CrossEncoder (ms-marco-MiniLM-L-6-v2)
   â””â”€â”€ Configure diversity settings (MMR)

5. INITIALIZE ORCHESTRATOR
   â”œâ”€â”€ Create EnhancedOrchestrator instance
   â”œâ”€â”€ Initialize all agents (Athena, Proteus, etc.)
   â”œâ”€â”€ Build LangGraph workflow
   â””â”€â”€ Compile state machine

6. INITIALIZE MEMORY SYSTEMS
   â”œâ”€â”€ MemoryManager (session context)
   â”œâ”€â”€ EpisodicMemory (long-term storage)
   â””â”€â”€ FeedbackStore (user ratings)

7. START UVICORN SERVER
   â””â”€â”€ Listen on 0.0.0.0:8000
```

### Frontend Initialization

```javascript
// App.jsx - on mount

1. CHECK AUTHENTICATION
   â”œâ”€â”€ Read localStorage('locallens_user')
   â”œâ”€â”€ If not logged in â†’ Show LoginSettings
   â””â”€â”€ If logged in â†’ Show ChatInterface

2. CHATINTERFACE MOUNT
   â”œâ”€â”€ Initialize state (messages, steps, query)
   â”œâ”€â”€ Load conversation history from API
   â””â”€â”€ Setup event handlers

3. DOCUMENT SELECTOR MOUNT
   â”œâ”€â”€ Fetch available documents
   â””â”€â”€ Restore attached documents (if any)
```

---

## Search Execution Flow

### Complete Request Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: USER INPUT                                              â”‚
â”‚ User types: "Find documents about machine learning"            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: FRONTEND PROCESSING (ChatInterface.jsx)                 â”‚
â”‚                                                                 â”‚
â”‚ handleSearch() {                                                â”‚
â”‚   1. Create user message object                                 â”‚
â”‚   2. Add to messages state                                      â”‚
â”‚   3. Clear input field                                          â”‚
â”‚   4. Set loading = true                                         â”‚
â”‚   5. POST to /search/enhanced:                                  â”‚
â”‚      {                                                          â”‚
â”‚        query: "Find documents about machine learning",          â”‚
â”‚        user_id: "user_1",                                       â”‚
â”‚        conversation_id: "conv_abc123",                          â”‚
â”‚        attached_documents: []                                   â”‚
â”‚      }                                                          â”‚
â”‚   6. Open SSE connection for real-time steps                    â”‚
â”‚ }                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: API LAYER (backend/api.py)                              â”‚
â”‚                                                                 â”‚
â”‚ @app.post("/search/enhanced")                                   â”‚
â”‚ async def enhanced_search(request):                             â”‚
â”‚   1. Extract query, user_id, conversation_id                    â”‚
â”‚   2. Get step_queue for SSE streaming                           â”‚
â”‚   3. Call orchestrator.process(query, context)                  â”‚
â”‚   4. Return: {results, response_message, confidence, steps}     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: ORCHESTRATOR (backend/orchestration/orchestrator.py)    â”‚
â”‚                                                                 â”‚
â”‚ class EnhancedOrchestrator:                                     â”‚
â”‚   async def process(query, context):                            â”‚
â”‚     1. Check attached_documents                                 â”‚
â”‚       - If attached â†’ route_to_daedalus()                       â”‚
â”‚       - If empty â†’ continue to Athena path                      â”‚
â”‚                                                                 â”‚
â”‚     2. Load session context from MemoryManager                  â”‚
â”‚       - Previous queries                                        â”‚
â”‚       - Relevant memories                                       â”‚
â”‚                                                                 â”‚
â”‚     3. Execute LangGraph workflow:                              â”‚
â”‚       classify_node â†’ search_node â†’ explain_node â†’ finalize     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: CLASSIFICATION (Athena - query_classifier.py)           â”‚
â”‚                                                                 â”‚
â”‚ QueryClassifier.classify(query):                                â”‚
â”‚   1. Rule-based classification:                                 â”‚
â”‚      - Check for keywords: "find", "show", "list" â†’ INFO_SEEK   â”‚
â”‚      - Check for keywords: "compare", "vs" â†’ COMPARISON         â”‚
â”‚      - Check for file patterns: ".pdf", ".docx" â†’ SPECIFIC_DOC  â”‚
â”‚                                                                 â”‚
â”‚   2. If confidence < 0.8, use LLM:                              â”‚
â”‚      prompt = "Classify this query: {query}"                    â”‚
â”‚      response = await call_ollama_json(prompt)                  â”‚
â”‚                                                                 â”‚
â”‚   3. Return:                                                    â”‚
â”‚      {                                                          â”‚
â”‚        intent: "INFO_SEEKING",                                  â”‚
â”‚        confidence: 0.85,                                        â”‚
â”‚        entities: ["machine learning"],                          â”‚
â”‚        should_search: true                                      â”‚
â”‚      }                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 6: STRATEGY SELECTION (Proteus - adaptive_retriever.py)    â”‚
â”‚                                                                 â”‚
â”‚ AdaptiveRetriever.select_strategy(query, intent):               â”‚
â”‚   1. Analyze query characteristics:                             â”‚
â”‚      - Length (short vs long)                                   â”‚
â”‚      - Specificity (entity names, dates)                        â”‚
â”‚      - Question type (factual vs exploratory)                   â”‚
â”‚                                                                 â”‚
â”‚   2. Select strategy:                                           â”‚
â”‚      "machine learning" â†’ BROAD (conceptual topic)              â”‚
â”‚                                                                 â”‚
â”‚   3. Return:                                                    â”‚
â”‚      {                                                          â”‚
â”‚        strategy: "broad",                                       â”‚
â”‚        vector_weight: 0.7,                                      â”‚
â”‚        bm25_weight: 0.3,                                        â”‚
â”‚        top_k: 50                                                â”‚
â”‚      }                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 7: HYBRID SEARCH (opensearch_client.py)                    â”‚
â”‚                                                                 â”‚
â”‚ OpenSearchClient.hybrid_search(query, vector, top_k, filters):  â”‚
â”‚                                                                 â”‚
â”‚   1. Generate query embedding:                                  â”‚
â”‚      vector = embedding_model.encode(query)  # [768 dims]       â”‚
â”‚                                                                 â”‚
â”‚   2. Vector search (k-NN):                                      â”‚
â”‚      POST /{index}/_search                                      â”‚
â”‚      {                                                          â”‚
â”‚        "query": {                                               â”‚
â”‚          "knn": {                                               â”‚
â”‚            "vector_embedding": {                                â”‚
â”‚              "vector": [0.123, -0.456, ...],                    â”‚
â”‚              "k": 50                                            â”‚
â”‚            }                                                    â”‚
â”‚          }                                                      â”‚
â”‚        }                                                        â”‚
â”‚      }                                                          â”‚
â”‚                                                                 â”‚
â”‚   3. BM25 text search:                                          â”‚
â”‚      POST /{index}/_search                                      â”‚
â”‚      {                                                          â”‚
â”‚        "query": {                                               â”‚
â”‚          "multi_match": {                                       â”‚
â”‚            "query": "machine learning",                         â”‚
â”‚            "fields": ["detailed_summary^3", "full_content^2"]   â”‚
â”‚          }                                                      â”‚
â”‚        }                                                        â”‚
â”‚      }                                                          â”‚
â”‚                                                                 â”‚
â”‚   4. Reciprocal Rank Fusion:                                    â”‚
â”‚      for each doc:                                              â”‚
â”‚        score = (vector_weight / (k + vector_rank + 1)) +        â”‚
â”‚                (bm25_weight / (k + bm25_rank + 1))              â”‚
â”‚                                                                 â”‚
â”‚   5. Return top 50 combined results                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 8: RERANKING (reranker.py)                                 â”‚
â”‚                                                                 â”‚
â”‚ CrossEncoderReranker.rerank(query, documents, top_k=10):        â”‚
â”‚                                                                 â”‚
â”‚   1. Create query-doc pairs:                                    â”‚
â”‚      pairs = [(query, doc.summary) for doc in documents]        â”‚
â”‚                                                                 â”‚
â”‚   2. Score with cross-encoder:                                  â”‚
â”‚      scores = cross_encoder.predict(pairs)                      â”‚
â”‚                                                                 â”‚
â”‚   3. Apply diversity (MMR):                                     â”‚
â”‚      - Avoid similar documents scoring high                     â”‚
â”‚      - Balance relevance vs diversity                           â”‚
â”‚                                                                 â”‚
â”‚   4. Apply feedback boosts (if user_id provided):               â”‚
â”‚      boosts = feedback_store.get_boosts(user_id, query)         â”‚
â”‚      for doc in results:                                        â”‚
â”‚        doc.score += boosts.get(doc.id, 0)                       â”‚
â”‚                                                                 â”‚
â”‚   5. Return top 10 reranked results                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 9: QUALITY EVALUATION (Diogenes - critic_agent.py)         â”‚
â”‚                                                                 â”‚
â”‚ CriticAgent.evaluate_quality(query, results):                   â”‚
â”‚   1. Check result count                                         â”‚
â”‚   2. Analyze relevance scores                                   â”‚
â”‚   3. Detect potential hallucination risk                        â”‚
â”‚   4. Score overall quality (0.0 - 1.0)                          â”‚
â”‚                                                                 â”‚
â”‚   Return: {quality_score: 0.82, issues: [], suggestions: []}    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 10: EXPLANATION (Hermes - explanation_agent.py)            â”‚
â”‚                                                                 â”‚
â”‚ ExplanationAgent.explain_results(query, results):               â”‚
â”‚   1. Generate natural language response                         â”‚
â”‚   2. Explain why results are relevant                           â”‚
â”‚   3. Add suggested follow-up questions                          â”‚
â”‚                                                                 â”‚
â”‚   Return: "I found 10 documents about machine learning..."      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 11: CONFIDENCE SCORING (Themis - confidence_scorer.py)     â”‚
â”‚                                                                 â”‚
â”‚ ConfidenceScorer.score(query, results, quality):                â”‚
â”‚   1. Evidence strength assessment                               â”‚
â”‚   2. Source count weighting                                     â”‚
â”‚   3. Query-result alignment check                               â”‚
â”‚                                                                 â”‚
â”‚   Return: {                                                     â”‚
â”‚     confidence: 0.85,                                           â”‚
â”‚     evidence_strength: "strong",                                â”‚
â”‚     supporting_sources: 8                                       â”‚
â”‚   }                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 12: RESPONSE ASSEMBLY                                      â”‚
â”‚                                                                 â”‚
â”‚ Final response to frontend:                                     â”‚
â”‚ {                                                               â”‚
â”‚   "results": [                                                  â”‚
â”‚     {                                                           â”‚
â”‚       "id": "abc123",                                           â”‚
â”‚       "filename": "ML_Guide.pdf",                               â”‚
â”‚       "detailed_summary": "This document explains...",          â”‚
â”‚       "score": 0.92,                                            â”‚
â”‚       "file_path": "C:/Documents/ML_Guide.pdf",                 â”‚
â”‚       "file_type": ".pdf",                                      â”‚
â”‚       "entities": ["neural networks", "deep learning"],         â”‚
â”‚       "keywords": "machine learning, AI, algorithms"            â”‚
â”‚     },                                                          â”‚
â”‚     ...                                                         â”‚
â”‚   ],                                                            â”‚
â”‚   "response_message": "Found 10 documents about ML...",         â”‚
â”‚   "confidence": 0.85,                                           â”‚
â”‚   "evidence_strength": {"level": "strong", "sources": 8},       â”‚
â”‚   "suggested_followups": ["What are neural networks?", ...],    â”‚
â”‚   "steps": [                                                    â”‚
â”‚     {"agent": "Athena", "action": "Classifying query..."},      â”‚
â”‚     {"agent": "Proteus", "action": "Selecting strategy..."},    â”‚
â”‚     ...                                                         â”‚
â”‚   ],                                                            â”‚
â”‚   "search_time": 1.23                                           â”‚
â”‚ }                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 13: FRONTEND RENDER (ChatInterface.jsx)                    â”‚
â”‚                                                                 â”‚
â”‚ 1. Add assistant message to messages state                      â”‚
â”‚ 2. Render ResultCard/ResultItem for each result                 â”‚
â”‚ 3. Display confidence badge                                     â”‚
â”‚ 4. Show thinking steps (collapsible)                            â”‚
â”‚ 5. Render follow-up suggestions as clickable buttons            â”‚
â”‚ 6. Set loading = false                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Document Ingestion Flow

### File Processing Sequence

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TRIGGER: User clicks "Start Indexing" in SettingsPanel          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ POST /index/start {directory: "C:/Documents"}                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ IngestionPipeline.process_directory(directory, task_id):        â”‚
â”‚                                                                 â”‚
â”‚   1. DISCOVER FILES                                             â”‚
â”‚      files = directory.rglob("*.pdf", "*.docx", ...)            â”‚
â”‚      total = len(files)  # e.g., 150 files                      â”‚
â”‚                                                                 â”‚
â”‚   2. BATCH PROCESSING (batch_size = 5)                          â”‚
â”‚      for batch in chunks(files, 5):                             â”‚
â”‚        await process_batch(batch)                               â”‚
â”‚        emit_progress(processed / total)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ IngestionPipeline.process_file(file_path):                      â”‚
â”‚                                                                 â”‚
â”‚   1. CHECK IF EXISTS                                            â”‚
â”‚      doc_id = md5(file_path)                                    â”‚
â”‚      if await opensearch.document_exists(doc_id):               â”‚
â”‚        return {"status": "skipped"}                             â”‚
â”‚                                                                 â”‚
â”‚   2. EXTRACT CONTENT (based on file type)                       â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚      â”‚ .pdf  â†’ PdfReader.pages[].extract_text()               â”‚ â”‚
â”‚      â”‚ .docx â†’ Document(file).paragraphs                      â”‚ â”‚
â”‚      â”‚ .txt  â†’ file.read()                                    â”‚ â”‚
â”‚      â”‚ .xlsx â†’ pd.read_excel() â†’ DataFrame.to_string()        â”‚ â”‚
â”‚      â”‚ .png  â†’ Ollama vision: "Describe this image"           â”‚ â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚   3. GENERATE SUMMARY (LLM)                                     â”‚
â”‚      prompt = """                                               â”‚
â”‚        Analyze this document and provide:                       â”‚
â”‚        SUMMARY: [detailed summary]                              â”‚
â”‚        KEYWORDS: [comma-separated]                              â”‚
â”‚        ENTITIES: [people, orgs, locations]                      â”‚
â”‚        TOPICS: [main topics]                                    â”‚
â”‚        RELATIONSHIPS: [entity1 -> relation -> entity2]          â”‚
â”‚      """                                                        â”‚
â”‚      response = await call_ollama(prompt, content[:8000])       â”‚
â”‚                                                                 â”‚
â”‚   4. PARSE RESPONSE                                             â”‚
â”‚      parsed = _parse_detailed_response(response)                â”‚
â”‚      # Extract: summary, keywords, entities, topics, relations  â”‚
â”‚                                                                 â”‚
â”‚   5. GENERATE EMBEDDING                                         â”‚
â”‚      embedding = embedding_model.encode(summary)                â”‚
â”‚      # Returns: [768 float values]                              â”‚
â”‚                                                                 â”‚
â”‚   6. BUILD DOCUMENT OBJECT                                      â”‚
â”‚      document = {                                               â”‚
â”‚        "id": doc_id,                                            â”‚
â”‚        "filename": "report.pdf",                                â”‚
â”‚        "file_path": "C:/Documents/report.pdf",                  â”‚
â”‚        "file_type": ".pdf",                                     â”‚
â”‚        "detailed_summary": "This report covers...",             â”‚
â”‚        "full_content": "[first 50000 chars]",                   â”‚
â”‚        "keywords": "quarterly, revenue, growth",                â”‚
â”‚        "entities": ["Q3 2024", "Sales Dept"],                   â”‚
â”‚        "topics": ["finance", "reporting"],                      â”‚
â”‚        "vector_embedding": [0.123, -0.456, ...],                â”‚
â”‚        "word_count": 5230,                                      â”‚
â”‚        "page_count": 12,                                        â”‚
â”‚        "file_size_bytes": 1048576,                              â”‚
â”‚        "created_at": "2024-01-15T10:30:00Z"                     â”‚
â”‚      }                                                          â”‚
â”‚                                                                 â”‚
â”‚   7. INDEX TO OPENSEARCH                                        â”‚
â”‚      await opensearch.index_document(document)                  â”‚
â”‚                                                                 â”‚
â”‚   8. INDEX TO KNOWLEDGE GRAPH                                   â”‚
â”‚      if entities or relationships:                              â”‚
â”‚        await _index_to_knowledge_graph(                         â”‚
â”‚          doc_id, filename, entities, relationships              â”‚
â”‚        )                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Agent Execution Details

### Athena (Query Classifier)

```python
# Intent Classification Logic

INTENT_PATTERNS = {
    "SPECIFIC_DOCUMENT": [
        r"find.*specific",
        r"show me.*file",
        r"open.*document"
    ],
    "INFORMATION_SEEKING": [
        r"what is",
        r"how does",
        r"explain"
    ],
    "COMPARISON": [
        r"compare",
        r"difference between",
        r"vs\.?"
    ],
    "AGGREGATION": [
        r"list all",
        r"show me all",
        r"how many"
    ]
}

# Classification flow:
1. Try rule-based matching (fast, no LLM)
2. If no match or low confidence â†’ LLM classification
3. Return intent + confidence + extracted entities
```

### Daedalus (Document Mode)

```python
# When user has attached documents

DAEDALUS_PIPELINE = [
    "Prometheus: Extract key content from attached docs",
    "Hypatia: Analyze content for the specific query",
    "Mnemosyne: Check memory for related context",
    "Daedalus: Synthesize final response"
]

# Process:
1. Load full content of attached documents from OpenSearch
2. Build focused context (summaries + relevant sections)
3. Generate response using only attached document context
4. Return with "document_mode": true flag
```

---

## Frontend Component Flow

### ChatInterface State Machine

```
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚    IDLE      â”‚
                 â”‚ (query="")   â”‚
                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ User types
                        â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   TYPING     â”‚
                 â”‚ (query="...")â”‚
                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ Submit
                        â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   LOADING    â”‚
                 â”‚ (loading=T)  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
                        â”‚ Response          â”‚ Retry
                        â–¼                   â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
                 â”‚   DISPLAY    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ (results)    â”‚
                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚ New query
                        â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚    IDLE      â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### EntityGraphModal Flow

```javascript
// User clicks ğŸ“Š Graph button

1. handleViewGraph(docId, filename)
   â””â”€â”€ setGraphModal({isOpen: true, documentId, documentName})

2. EntityGraphModal mounts
   â””â”€â”€ useEffect â†’ fetchEntityData()

3. GET /documents/{id}/entities
   â””â”€â”€ Returns: {entities, keywords, topics, graph}

4. drawGraph(canvas, graph)
   â”œâ”€â”€ Position nodes radially
   â”œâ”€â”€ Draw edges (lines)
   â””â”€â”€ Draw nodes (circles with labels)

5. Render legend + tag cloud
```

---

## Database Schemas

### OpenSearch Index (locallens_docs)

```json
{
  "mappings": {
    "properties": {
      "id": {"type": "keyword"},
      "filename": {"type": "text", "fields": {"keyword": {"type": "keyword"}}},
      "file_path": {"type": "keyword"},
      "file_type": {"type": "keyword"},
      "content_type": {"type": "keyword"},
      "document_type": {"type": "keyword"},
      "is_image": {"type": "boolean"},
      "detailed_summary": {"type": "text", "analyzer": "content_analyzer"},
      "full_content": {"type": "text", "analyzer": "content_analyzer"},
      "keywords": {"type": "text"},
      "entities": {"type": "keyword"},
      "topics": {"type": "keyword"},
      "vector_embedding": {
        "type": "knn_vector",
        "dimension": 768,
        "method": {"name": "hnsw", "space_type": "innerproduct"}
      },
      "word_count": {"type": "integer"},
      "page_count": {"type": "integer"},
      "file_size_bytes": {"type": "long"},
      "created_at": {"type": "date"},
      "last_modified": {"type": "date"}
    }
  }
}
```

### SQLite: Conversations (locallens_conversations.db)

```sql
CREATE TABLE conversations (
    id TEXT PRIMARY KEY,
    user_id TEXT NOT NULL,
    title TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE messages (
    id TEXT PRIMARY KEY,
    conversation_id TEXT NOT NULL,
    role TEXT NOT NULL,  -- 'user' or 'assistant'
    content TEXT NOT NULL,
    metadata TEXT,  -- JSON: results, confidence, steps
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (conversation_id) REFERENCES conversations(id)
);
```

### SQLite: Feedback (locallens.db)

```sql
CREATE TABLE feedback (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    user_id TEXT NOT NULL,
    query TEXT NOT NULL,
    document_id TEXT NOT NULL,
    feedback_score INTEGER,  -- 1 (helpful) or -1 (not helpful)
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### SQLite: Knowledge Graph (locallens_graph.db)

```sql
CREATE TABLE entities (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    entity_type TEXT NOT NULL,
    properties TEXT,  -- JSON
    document_ids TEXT,  -- JSON array
    created_at TIMESTAMP
);

CREATE TABLE relationships (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    source_id TEXT NOT NULL,
    target_id TEXT NOT NULL,
    relationship_type TEXT NOT NULL,
    weight REAL DEFAULT 1.0,
    properties TEXT,
    document_id TEXT
);
```

---

## API Request/Response Examples

### Search Request

```bash
curl -X POST http://localhost:8000/search/enhanced \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Find documents about machine learning",
    "user_id": "user_123",
    "conversation_id": "conv_456",
    "attached_documents": []
  }'
```

### Search Response

```json
{
  "results": [
    {
      "id": "a1b2c3d4",
      "filename": "ML_Handbook.pdf",
      "detailed_summary": "Comprehensive guide to machine learning covering supervised learning, neural networks, and practical applications...",
      "score": 0.92,
      "file_path": "C:/Documents/ML_Handbook.pdf",
      "file_type": ".pdf",
      "file_size_bytes": 2456789,
      "entities": ["neural networks", "TensorFlow", "scikit-learn"],
      "keywords": "machine learning, AI, deep learning, algorithms"
    }
  ],
  "response_message": "I found 8 documents related to machine learning. The most relevant is 'ML_Handbook.pdf' which provides a comprehensive overview...",
  "confidence": 0.88,
  "evidence_strength": {
    "level": "strong",
    "supporting_sources": 6
  },
  "suggested_followups": [
    "What are the main types of machine learning?",
    "How does deep learning differ from traditional ML?"
  ],
  "steps": [
    {
      "agent": "Athena",
      "action": "Classified as information-seeking query",
      "timestamp": "2024-01-15T10:30:01Z"
    },
    {
      "agent": "Proteus",
      "action": "Selected 'broad' retrieval strategy",
      "timestamp": "2024-01-15T10:30:02Z"
    }
  ],
  "search_time": 1.45
}
```

---

## Error Handling & Fallbacks

### Search Fallback Chain

```
1. Hybrid Search Fails
   â””â”€â”€ Fallback: Vector-only search

2. Vector Search Fails
   â””â”€â”€ Fallback: BM25-only search

3. All Search Fails
   â””â”€â”€ Return: "No results found"

4. LLM Classification Fails
   â””â”€â”€ Fallback: Rule-based classification

5. Reranker Fails
   â””â”€â”€ Fallback: Return unreranked results

6. Response Generation Fails
   â””â”€â”€ Fallback: "Found {n} results for your query"
```

### LLM Response Sanitization

```python
# sanitize_llm_response() handles:
1. Markdown code blocks: ```json ... ```
2. Text before/after JSON
3. Multiple JSON objects
4. Empty responses â†’ {}
5. Invalid JSON â†’ original text
```

---

<p align="center">
  <i>Document version: 2024-12-17</i>
</p>
