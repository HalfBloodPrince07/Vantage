# LocaLense_V2 ‚Äì Comprehensive Project Documentation

---

## Table of Contents
1. [Project Overview](#project-overview)
2. [Technology Stack](#technology-stack)
3. [Repository Structure](#repository-structure)
4. [Backend Architecture](#backend-architecture)
   - [API Endpoints](#api-endpoints)
   - [Orchestrator (Zeus)](#orchestrator-zeus)
   - [Agents](#agents)
   - [Memory & Ingestion](#memory--ingestion)
   - [Utilities](#utilities)
5. [Frontend Architecture](#frontend-architecture)
   - [React Components](#react-components)
   - [Server‚ÄëSent Events (SSE) Flow](#sse-flow)
6. [Data Flow Diagram](#data-flow-diagram)
7. [Configuration](#configuration)
8. [Running the Application](#running-the-application)
9. [Deployment (Docker Compose)](#deployment-docker-compose)
10. [Testing & Verification](#testing--verification)
11. [Extending the System](#extending-the-system)
12. [Troubleshooting & FAQ](#troubleshooting--faq)
13. [License](#license)

---

## Project Overview
`LocaLense_V2` (also called **Vantage**) is an AI‚Äëpowered semantic document search and chat assistant. It lets users:
- Securely store and index personal documents (PDF, Word, Excel, images, plain text, etc.).
- Perform natural‚Äëlanguage searches across their own files using hybrid vector + BM25 retrieval.
- Attach documents to a conversation and ask follow‚Äëup questions that are answered using a multi‚Äëagent workflow inspired by Greek mythology.
- Get explanations, summarizations, comparisons, and quality‚Äëchecked results.

The system is built around a **Zeus** orchestrator that routes every request to the appropriate specialist agents (Athena, Daedalus, Prometheus, etc.).

---

## Technology Stack
| Layer | Technology | Purpose |
|-------|------------|---------|
| **Frontend** | React 18 + Vite | Modern SPA with hot‚Äëreload, dark mode, and responsive UI |
| **Backend** | FastAPI (Python) | Async REST API + Server‚ÄëSent Events (SSE) |
| **Vector DB** | OpenSearch 2.x | Hybrid vector + BM25 search |
| **LLM** | Ollama (`qwen2.5:7b`) | Local LLM for reasoning, classification, and generation |
| **Embeddings** | `nomic-embed-text` | 768‚Äëdimensional text embeddings |
| **Storage** | SQLite | Users, conversations, memory |
| **Agent Framework** | LangGraph (optional) | State‚Äëful graph orchestration |
| **Python Dependencies** | See `requirements.txt` |
| **Frontend Dependencies** | React, Vite, Lucide‚Äëreact, react‚Äëmarkdown |

---

## Repository Structure
```
LocaLense_V2/
‚îú‚îÄ‚îÄ backend/                     # FastAPI server and all agents
‚îÇ   ‚îú‚îÄ‚îÄ api.py                  # Main FastAPI app (routes, SSE)
‚îÇ   ‚îú‚îÄ‚îÄ auth_endpoints.py       # JWT auth (register, login, me)
‚îÇ   ‚îú‚îÄ‚îÄ ingestion.py            # Document extraction & chunking
‚îÇ   ‚îú‚îÄ‚îÄ memory/                 # Session & user memory manager
‚îÇ   ‚îú‚îÄ‚îÄ orchestration/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ orchestrator.py      # ‚ö° Zeus ‚Äì central orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ agents/                 # Specialist agents
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ query_classifier.py # ü¶â Athena ‚Äì intent classification
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ analysis_agent.py   # üìä Aristotle ‚Äì document analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clarification_agent.py # ü§î Socrates ‚Äì clarification
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ summarization_agent.py # üìú Thoth ‚Äì summarization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ explanation_agent.py   # üì® Hermes ‚Äì explanations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ critic_agent.py        # üîé Diogenes ‚Äì quality control
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ document_agents/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ daedalus_orchestrator.py # üèõÔ∏è Daedalus ‚Äì document pipeline
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ prometheus_reader.py   # üî• Prometheus ‚Äì text extraction
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ hypatia_analyzer.py    # üìö Hypatia ‚Äì semantic analysis
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ mnemosyne_extractor.py # üß† Mnemosyne ‚Äì insights
‚îÇ   ‚îú‚îÄ‚îÄ utils/                  # Helper modules (LLM utils, OpenSearch client, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ tests/                  # Unit / integration tests
‚îú‚îÄ‚îÄ frontend/                   # Vite + React UI
‚îÇ   ‚îî‚îÄ‚îÄ src/components/         # UI components (ChatInterface, etc.)
‚îú‚îÄ‚îÄ config.yaml                 # Global configuration (Ollama, OpenSearch, agents)
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ README.md                   # High‚Äëlevel README (this file expands it)
‚îú‚îÄ‚îÄ ARCHITECTURE.md            # Detailed architecture description
‚îú‚îÄ‚îÄ QUICKSTART.md               # Quick‚Äëstart guide (install & run)
‚îî‚îÄ‚îÄ working.md                  # Working document (auto‚Äëgenerated by this task)
```

---

## Backend Architecture
### API Endpoints
Key FastAPI routes (see `backend/api.py`):
- `POST /auth/register` ‚Äì Create a new user (hashed password stored in `locallens_users.json`).
- `POST /auth/login` ‚Äì Issue JWT token.
- `GET /auth/me` ‚Äì Return current user profile.
- `POST /search/enhanced` ‚Äì Main entry point for a query. Accepts optional `attached_documents` list.
- `POST /index/directory` ‚Äì Index a folder path (triggers `ingestion.py`).
- `POST /upload` ‚Äì Upload a single file for immediate indexing.
- SSE endpoint `/events` streams step‚Äëby‚Äëstep agent thinking to the UI.

### Orchestrator (**Zeus**)
Implemented in `backend/orchestration/orchestrator.py`. Responsibilities:
1. Load session context & memory.
2. Classify intent via **Athena** (`query_classifier`).
3. Route to **Athena Path** (general queries) or **Daedalus Path** (document‚Äëattached queries).
4. Execute a LangGraph workflow when available, otherwise fall back to a simplified async pipeline.
5. Record each step with `emit_step` so the frontend can display the thinking chain.
6. Persist interaction details in `MemoryManager` (SQLite).

### Agents
| Agent | File | Role |
|-------|------|------|
| **Athena** (ü¶â) | `backend/agents/query_classifier.py` | Intent classification, entity extraction, filter generation |
| **Aristotle** (üìä) | `backend/agents/analysis_agent.py` | Document comparison & insight generation |
| **Socrates** (ü§î) | `backend/agents/clarification_agent.py` | Generate clarification questions for ambiguous queries |
| **Thoth** (üìú) | `backend/agents/summarization_agent.py` | Multi‚Äëdocument summarization |
| **Hermes** (üì®) | `backend/agents/explanation_agent.py` | Explain relevance / ranking of results |
| **Diogenes** (üîé) | `backend/agents/critic_agent.py` | Quality evaluation & reformulation suggestions |
| **Daedalus** (üèõÔ∏è) | `backend/agents/document_agents/daedalus_orchestrator.py` | Orchestrates the document pipeline when files are attached |
| **Prometheus** (üî•) | `backend/agents/document_agents/prometheus_reader.py` | Extract raw text from PDFs, images, etc. |
| **Hypatia** (üìö) | `backend/agents/document_agents/hypatia_analyzer.py` | Semantic analysis, keyword extraction |
| **Mnemosyne** (üß†) | `backend/agents/document_agents/mnemosyne_extractor.py` | Insight extraction, fact synthesis |

### Memory & Ingestion
- **MemoryManager** (`backend/memory`) stores per‚Äësession context, recent queries, and user preferences in SQLite (`locallens_memory.db`).
- **Ingestion** (`backend/ingestion.py`) extracts text, chunks it, generates embeddings via `nomic-embed-text`, and indexes into OpenSearch (`documents` index). It also updates the UI via SSE progress events.

### Utilities
- `backend/utils/llm_utils.py` ‚Äì Wrapper around Ollama API (`call_ollama_with_retry`, `call_ollama_json`).
- `backend/utils/opensearch_client.py` ‚Äì Thin client for hybrid search (`vector_weight`, `bm25_weight`).
- `backend/utils/logger.py` ‚Äì Centralised `loguru` logger configuration.

---

## Frontend Architecture
The UI lives in `frontend/` and is built with Vite.

### React Components
- **ChatInterface.jsx** ‚Äì Main chat window, handles user input, displays messages, and renders the step‚Äëby‚Äëstep thinking chain received via SSE.
- **ChatInterface.css** ‚Äì Styling (dark mode, glass‚Äëmorphism, micro‚Äëanimations).
- **Auth Pages** ‚Äì Sign‚ÄëUp / Sign‚ÄëIn components that store the JWT in `localStorage` and attach it to API calls.
- **IndexPanel.jsx** ‚Äì UI for selecting a folder path or uploading files.

### Server‚ÄëSent Events (SSE) Flow
1. The frontend opens an `EventSource` to `/events` after a query is submitted.
2. The backend emits JSON objects `{type: "step", agent, action, details, timestamp}` for each agent step.
3. The UI appends a visual card for each step, using icons defined in the agent metadata.
4. When the final response arrives, the UI shows the assistant message and any attached sources.

---

## Data Flow Diagram
```mermaid
sequenceDiagram
    participant U as User (Browser)
    participant F as Frontend (React)
    participant B as Backend (FastAPI)
    participant Z as Zeus (Orchestrator)
    participant A as Athena (Intent Classifier)
    participant D as Daedalus (Document Pipeline)
    participant P as Prometheus (Extract Text)
    participant H as Hypatia (Semantic Analysis)
    participant M as Mnemosyne (Insights)
    participant S as Search (OpenSearch)
    participant L as LLM (Ollama)
    participant Q as Queue / Memory

    U->>F: Type query (optional docs attached)
    F->>B: POST /search/enhanced
    B->>Z: process_query()
    Z->>A: classify intent
    alt Document attached
        Z->>D: route_to_daedalus
        D->>P: extract text
        D->>H: semantic analysis
        D->>M: insight extraction
        D->>Z: answer + sources
    else No docs
        Z->>S: hybrid vector + BM25 search
        S->>Z: results
        Z->>L: generate explanation (Hermes)
        Z->>Q: quality check (Diogenes)
    end
    Z->>B: final response
    B->>F: SSE stream of steps + answer
    F->>U: Render chat UI
```

---

## Configuration
Key sections of `config.yaml` (excerpt):
```yaml
ollama:
  base_url: "http://localhost:11434"
  text_model:
    name: "qwen2.5:7b"
    temperature: 0.7
  vision_model:
    name: "qwen2.5vl:latest"

opensearch:
  host: "localhost"
  port: 9200
  index:
    documents: "documents"
    conversations: "conversations"

search:
  hybrid:
    enabled: true
    vector_weight: 0.7
    bm25_weight: 0.3

agents:
  classifier:
    enabled: true
  clarification:
    enabled: true
  analysis:
    enabled: true
  summarization:
    enabled: true
  explanation:
    enabled: true
  critic:
    enabled: true
```
- Adjust `ollama.base_url` if Ollama runs on a different host/port.
- OpenSearch can be run via Docker (`docker-compose.yml`).
- Toggle individual agents via the `agents` block.

---

## Running the Application
### Prerequisites
- Python‚ÄØ3.10+ (`pip install -r requirements.txt`)
- Node.js‚ÄØ18+ (`cd frontend && npm install`)
- Docker (for OpenSearch) **or** a running OpenSearch instance.
- Ollama with the required models (`ollama serve`, `ollama pull nomic-embed-text`, `ollama pull qwen2.5:7b`).

### Quick Start (Windows) ‚Äì `run.bat`
```bat
:: Start OpenSearch (Docker)
docker-compose up -d

:: Start Ollama (ensure it is in PATH)
ollama serve
ollama pull nomic-embed-text
ollama pull qwen2.5:7b

:: Start Backend
python -m backend.api

:: Start Frontend
cd frontend && npm run dev
```
Open your browser at `http://localhost:5173`.

---

## Deployment (Docker Compose)
`docker-compose.yml` defines services for:
- `opensearch` ‚Äì the vector DB.
- `backend` ‚Äì FastAPI container (exposes port‚ÄØ8000).
- `frontend` ‚Äì Vite dev server (exposes port‚ÄØ5173).
- `ollama` ‚Äì optional container if you prefer to run Ollama inside Docker.

To deploy:
```bash
docker compose up -d
# Backend will automatically start after OpenSearch is healthy.
```
Environment variables can be overridden via a `.env` file (e.g., `OLLAMA_BASE_URL`).

---

## Testing & Verification
- Unit tests live in `backend/tests/`. Run with `pytest backend/tests/`.
- End‚Äëto‚Äëend manual test:
  1. Register a user.
  2. Index a folder containing a few PDFs.
  3. Ask a query like `"show me invoices from last month"`.
  4. Verify the step‚Äëby‚Äëstep UI shows agents Athena ‚Üí Search ‚Üí Hermes ‚Üí Diogenes ‚Üí Zeus.
  5. Check the SQLite `locallens_memory.db` for stored conversation history.

---

## Extending the System
- **Add a new agent**: create a class in `backend/agents/`, register it in `EnhancedOrchestrator.__init__`, and add a node to the LangGraph workflow (or a fallback async function).
- **Support new file types**: extend `_extract_filters` in `query_classifier.py` and update `ingestion.py` with a parser (e.g., `pptx` via `python-pptx`).
- **Swap LLM**: modify `config.yaml` to point to a different Ollama model or an external API wrapper.
- **Front‚Äëend UI**: add new React components under `frontend/src/components/` and expose them via routes in `App.jsx`.

---

## Troubleshooting & FAQ
**OpenSearch not connecting?**
```bash
curl http://localhost:9200
docker-compose logs opensearch
```
**Ollama models missing?**
```bash
ollama list
ollama pull nomic-embed-text
ollama pull qwen2.5:7b
```
**Frontend fails to load?**
```bash
cd frontend
rm -rf node_modules package-lock.json
npm install && npm run dev
```
**Why is my query routed to Athena instead of Daedalus?**
Check that the request includes `attached_documents` (list of document IDs) and that the query contains document‚Äësearch keywords.

---

## License
MIT License

---

*Made with ‚ù§Ô∏è using AI‚Äëpowered semantic search and the wisdom of the Greek Pantheon.*
