# CriticAgent (Answer Critique) Architecture

**File:** `backend/agents/critic_agent.py`

---

## Purpose

The CriticAgent evaluates the **quality and safety** of the answer generated by the LLM. It checks for factual correctness, policy violations, hallucinations, and tone appropriateness, returning a critique that can be used to revise the answer or to warn the user.

---

## High‚ÄëLevel ASCII Diagram

```
+-------------------+      +----------------------+      +-------------------+
|   Final Answer    | ---> |   CriticAgent        | ---> |   Critique Report |
+-------------------+      +----------------------+      +-------------------+
        |                         |                         |
        | 1. Fact‚Äëcheck          | 2. Policy scan          |
        | 3. Hallucination detect| 4. Tone analysis        |
        v                         v                         v
+-------------------+      +----------------------+      +-------------------+
|   Source Docs     |      |   Knowledge Base     |      |   JSON Report    |
+-------------------+      +----------------------+      +-------------------+
```

---

## Core Methods

```python
class CriticAgent:
    async def critique(self, answer: str, sources: List[Dict]) -> CritiqueReport:
        """Return a structured critique of the answer."""
        facts_ok = await self._fact_check(answer, sources)
        policy_ok = self._policy_check(answer)
        hallucination_score = await self._detect_hallucination(answer)
        tone_ok = self._tone_check(answer)
        return CritiqueReport(
            factual_correctness=facts_ok,
            policy_compliance=policy_ok,
            hallucination_score=hallucination_score,
            tone_ok=tone_ok,
        )

    async def _fact_check(self, answer: str, sources: List[Dict]) -> bool:
        """Cross‚Äëreference answer statements with source snippets."""
        ...

    def _policy_check(self, answer: str) -> bool:
        """Simple regex / keyword filter for disallowed content."""
        ...

    async def _detect_hallucination(self, answer: str) -> float:
        """LLM prompt that returns a hallucination probability (0‚Äë1)."""
        ...

    def _tone_check(self, answer: str) -> bool:
        """Detect overly aggressive or unprofessional language."""
        ...
```

---

## Interaction with Other Components

- **Zeus** calls `CriticAgent.critique` after the answer is generated. If the critique fails, Zeus may trigger a re‚Äëgeneration.
- **Themis** can incorporate the `hallucination_score` into its overall confidence.
- **MemoryManager** stores the critique for audit purposes.
- **Frontend** displays warnings or suggestions based on the critique.

---

## Configuration (`config.yaml`)

```yaml
agents:
  critic_agent:
    enabled: true
    policy_rules_path: "config/policy_rules.yaml"
    hallucination_prompt: |
      Evaluate the following answer for hallucinations and return a probability between 0 and 1.
    tone_rules:
      - "no profanity"
      - "professional tone"
```

---

## Error Handling & Logging

- Logs each check with `loguru` (`üõ°Ô∏è Critic`).
- If any external service fails, the agent returns a safe default (`factual_correctness=False`).
- Exceptions are caught and a minimal critique is returned so the workflow can continue.

---

## Testing Strategy

1. **Unit Tests** for `_policy_check` with known prohibited phrases.
2. **Mock LLM** for `_detect_hallucination` to verify probability parsing.
3. **Integration Test**: Full query flow, assert that a low hallucination score leads to no re‚Äëgeneration, while a high score triggers a retry.
4. **Performance**: Critique generation ‚â§‚ÄØ300‚ÄØms.

---

## Data Classes

```python
@dataclass
class CritiqueReport:
    factual_correctness: bool
    policy_compliance: bool
    hallucination_score: float  # 0‚Äë1
    tone_ok: bool
```

---

*CriticAgent safeguards Vantage‚Äôs outputs, ensuring answers are accurate, policy‚Äëcompliant, and trustworthy.*
